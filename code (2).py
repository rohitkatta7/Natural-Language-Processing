# -*- coding: utf-8 -*-
"""Copy of Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AfMgsj4hxQQgYb_lcvLiUrCyY4f_zJub
"""

import json
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize

# Download necessary NLTK resources
nltk.download('punkt')

# File paths for input and output
jsonl_path = 'train.jsonl'  # Path to the input JSONL file
txt_path = 'data.txt'  # Path for the output text file

# Convert JSONL to a plain text file
with open(jsonl_path, 'r') as input_file, open(txt_path, 'w') as output_file:
    for record in input_file:
        content = json.loads(record)
        if 'messages' in content and isinstance(content['messages'], list):
            for msg in content['messages']:
                output_file.write(msg + '\n')

# Analyzing the text data
with open(txt_path, 'r') as data_file:
    full_text = data_file.read()

# Split the text into individual sentences
all_sentences = sent_tokenize(full_text)
all_sentences = [sen for sen in all_sentences if sen.strip()]

# (a) Count the total number of sentences
total_sentences = len(all_sentences)
print(f"Total number of sentences: {total_sentences}")

# (b) Split sentences into words using the split method
split_tokens = [word for sentence in all_sentences for word in sentence.split(' ')]
total_split_tokens = len(split_tokens)
print(f"Tokens using split method: {total_split_tokens}")

# (c) Use NLTK's tokenizer to split sentences into words
nltk_tokens = [token for sentence in all_sentences for token in word_tokenize(sentence)]
total_nltk_tokens = len(nltk_tokens)
print(f"Tokens using NLTK tokenizer: {total_nltk_tokens}")

# (d) Convert all tokens to lowercase and count unique types
lower_tokens = [token.lower() for token in nltk_tokens]
total_lower_tokens = len(lower_tokens)
unique_lower_tokens = len(set(lower_tokens))
print(f"Lowercase tokens count: {total_lower_tokens}")
print(f"Unique lowercase types: {unique_lower_tokens}")

# (e) Token count comparison
print("Token count comparison:")
print(f"Split method tokens: {total_split_tokens}")
print(f"NLTK tokenizer tokens: {total_nltk_tokens}")
print(f"Lowercase tokens: {total_lower_tokens}")

# (f), (g), (h) Create a frequency dictionary for word types
type_frequencies = {}
for token in lower_tokens:
    type_frequencies[token] = type_frequencies.get(token, 0) + 1

# Sort the frequency dictionary
sorted_frequencies = sorted(type_frequencies.items(), key=lambda pair: pair[1], reverse=True)

# Most frequent and 5th most frequent word types
top_word = sorted_frequencies[0]
fifth_top_word = sorted_frequencies[4]
print(f"Most frequent word type: {top_word}")
print(f"5th most frequent word type: {fifth_top_word}")

# (h) Visualize word frequencies to examine Zipf's law
import matplotlib.pyplot as plt

# Prepare data for the plot
word_ranks = range(1, len(sorted_frequencies) + 1)
word_freqs = [freq for _, freq in sorted_frequencies]

# Generate the plot
plt.figure(figsize=(10, 6))
plt.loglog(word_ranks, word_freqs)
plt.title("Word Frequency Distribution")
plt.xlabel("Word Rank (log scale)")
plt.ylabel("Word Frequency (log scale)")
plt.show()